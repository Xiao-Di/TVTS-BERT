{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39798489, 0.43576826, 0.4256927 , ..., 0.49874055, 0.52141058,\n",
       "        1.        ],\n",
       "       [0.50125945, 0.61712846, 0.53400504, ..., 0.57178841, 0.58942065,\n",
       "        1.        ],\n",
       "       [0.58690176, 0.59949622, 0.697733  , ..., 0.38035264, 0.37783375,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.12195122, 0.1358885 , 0.14285714, ..., 0.79094077, 0.79094077,\n",
       "        0.        ],\n",
       "       [0.77351916, 0.77351916, 0.79442509, ..., 0.81533101, 0.79442509,\n",
       "        0.        ],\n",
       "       [0.77351916, 0.76655052, 0.76655052, ..., 0.17421603, 0.16376307,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/gengyunxin/Documents/项目/traffic_model/test/data/normalized_data_72.csv', header=0)\n",
    "Data = df.values\n",
    "TS_num = Data.shape[0]\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "# ts_length = 72\n",
    "seq_len = 72\n",
    "word_len = 6\n",
    "ts_data = Data[:, 0:-1]\n",
    "ts_processed = np.expand_dims(np.array(ts_data[index]), -1)\n",
    "class_label = np.array([Data[index][-1]], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_length = ts_processed.shape[0]\n",
    "num_words = int(ts_length / word_len)\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_mask = np.zeros((seq_len,), dtype=int)\n",
    "bert_mask[:ts_length] = 1\n",
    "bert_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39798489],\n",
       "       [0.43576826],\n",
       "       [0.4256927 ],\n",
       "       [0.39042821],\n",
       "       [0.40302267],\n",
       "       [0.39798489],\n",
       "       [0.38287154],\n",
       "       [0.46095718],\n",
       "       [0.45843829],\n",
       "       [0.47607053],\n",
       "       [0.50629723],\n",
       "       [0.38035264],\n",
       "       [0.5768262 ],\n",
       "       [0.58690176],\n",
       "       [0.44080605],\n",
       "       [0.54911839],\n",
       "       [0.45843829],\n",
       "       [0.54156171],\n",
       "       [0.52392947],\n",
       "       [0.45088161],\n",
       "       [0.50881612],\n",
       "       [0.53652393],\n",
       "       [0.47607053],\n",
       "       [0.55163728],\n",
       "       [0.4836272 ],\n",
       "       [0.4836272 ],\n",
       "       [0.48110831],\n",
       "       [0.4836272 ],\n",
       "       [0.44332494],\n",
       "       [0.51889169],\n",
       "       [0.41813602],\n",
       "       [0.53148615],\n",
       "       [0.51889169],\n",
       "       [0.47858942],\n",
       "       [0.4256927 ],\n",
       "       [0.42821159],\n",
       "       [0.45088161],\n",
       "       [0.37783375],\n",
       "       [0.50881612],\n",
       "       [0.50881612],\n",
       "       [0.44584383],\n",
       "       [0.41309824],\n",
       "       [0.38287154],\n",
       "       [0.44332494],\n",
       "       [0.42821159],\n",
       "       [0.40806045],\n",
       "       [0.40806045],\n",
       "       [0.40302267],\n",
       "       [0.4836272 ],\n",
       "       [0.5138539 ],\n",
       "       [0.51889169],\n",
       "       [0.49622166],\n",
       "       [0.48866499],\n",
       "       [0.40302267],\n",
       "       [0.40302267],\n",
       "       [0.43828715],\n",
       "       [0.4231738 ],\n",
       "       [0.43576826],\n",
       "       [0.48866499],\n",
       "       [0.49370277],\n",
       "       [0.53652393],\n",
       "       [0.53400504],\n",
       "       [0.52644836],\n",
       "       [0.50377834],\n",
       "       [0.53904282],\n",
       "       [0.54156171],\n",
       "       [0.53148615],\n",
       "       [0.58438287],\n",
       "       [0.49622166],\n",
       "       [0.58438287],\n",
       "       [0.49874055],\n",
       "       [0.52141058]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = 1\n",
    "ts_masking = ts_processed.copy()\n",
    "mask = np.zeros((seq_len,), dtype=int)\n",
    "\n",
    "for i in range(num_words):\n",
    "    prob = random.random()\n",
    "    if prob < 0.2:\n",
    "        prob /= 0.2\n",
    "        mask[(i * word_len):(i * word_len + 6)] = 1\n",
    "\n",
    "        if prob < 0.5:\n",
    "            ts_masking[(i * word_len):(i * word_len + 6), :] \\\n",
    "                += np.random.uniform(low=-0.5, high=0, size=(dimension,))\n",
    "        else:\n",
    "            ts_masking[(i * word_len):(i * word_len + 6), :] \\\n",
    "                += np.random.uniform(low=0, high=0.5, size=(dimension,))\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39798489],\n",
       "       [0.43576826],\n",
       "       [0.4256927 ],\n",
       "       [0.39042821],\n",
       "       [0.40302267],\n",
       "       [0.39798489],\n",
       "       [0.38287154],\n",
       "       [0.46095718],\n",
       "       [0.45843829],\n",
       "       [0.47607053],\n",
       "       [0.50629723],\n",
       "       [0.38035264],\n",
       "       [0.3533097 ],\n",
       "       [0.36338526],\n",
       "       [0.21728955],\n",
       "       [0.32560189],\n",
       "       [0.23492179],\n",
       "       [0.31804521],\n",
       "       [0.52392947],\n",
       "       [0.45088161],\n",
       "       [0.50881612],\n",
       "       [0.53652393],\n",
       "       [0.47607053],\n",
       "       [0.55163728],\n",
       "       [0.4836272 ],\n",
       "       [0.4836272 ],\n",
       "       [0.48110831],\n",
       "       [0.4836272 ],\n",
       "       [0.44332494],\n",
       "       [0.51889169],\n",
       "       [0.41813602],\n",
       "       [0.53148615],\n",
       "       [0.51889169],\n",
       "       [0.47858942],\n",
       "       [0.4256927 ],\n",
       "       [0.42821159],\n",
       "       [0.45088161],\n",
       "       [0.37783375],\n",
       "       [0.50881612],\n",
       "       [0.50881612],\n",
       "       [0.44584383],\n",
       "       [0.41309824],\n",
       "       [0.38287154],\n",
       "       [0.44332494],\n",
       "       [0.42821159],\n",
       "       [0.40806045],\n",
       "       [0.40806045],\n",
       "       [0.40302267],\n",
       "       [0.4836272 ],\n",
       "       [0.5138539 ],\n",
       "       [0.51889169],\n",
       "       [0.49622166],\n",
       "       [0.48866499],\n",
       "       [0.40302267],\n",
       "       [0.40302267],\n",
       "       [0.43828715],\n",
       "       [0.4231738 ],\n",
       "       [0.43576826],\n",
       "       [0.48866499],\n",
       "       [0.49370277],\n",
       "       [1.03344452],\n",
       "       [1.03092562],\n",
       "       [1.02336895],\n",
       "       [1.00069892],\n",
       "       [1.03596341],\n",
       "       [1.0384823 ],\n",
       "       [0.53148615],\n",
       "       [0.58438287],\n",
       "       [0.49622166],\n",
       "       [0.58438287],\n",
       "       [0.49874055],\n",
       "       [0.52141058]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_masking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = '/Users/gengyunxin/Documents/项目/traffic_model/test/data/normalized_data_72.csv'\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 file_path='/Users/gengyunxin/Documents/项目/traffic_model/test/data/normalized_data_72.csv',\n",
    "                 num_features = 1, seq_len=72, word_len=6):\n",
    "        self.seq_len = seq_len\n",
    "        self.word_len = word_len\n",
    "        self.dimension = num_features\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        self.Data = df.values # (5855,73)\n",
    "        print(\"Loading data successfully!\")\n",
    "        self.TS_num = self.Data.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.TS_num\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ts_data = self.Data[:, 0:-1] # 不含label的数组 (5855,72)\n",
    "        # Normalize\n",
    "        # max = ts_data.max() # 400.0\n",
    "        # min = ts_data.min() # 3.0\n",
    "        # ts_data_normalized = (ts_data - min) / (max - min)\n",
    "\n",
    "        ts_processed = np.expand_dims(np.array(ts_data[index]), -1) # 归一化后的数据 (72,1)\n",
    "        class_label = np.array([self.Data[index][-1]], dtype=int) # label (1,)\n",
    "\n",
    "        ts_length = ts_processed.shape[0] # 72\n",
    "        num_words = int(ts_length / self.word_len)\n",
    "\n",
    "        bert_mask = np.zeros((self.seq_len,), dtype=int)\n",
    "        bert_mask[:ts_length] = 1 # 其实seq_len和ts_length长度相同，都是72,全1数组。这么写是为了应对长度不等的情况 (72,)\n",
    "\n",
    "        # 随机噪声\n",
    "        ts_masking, mask = self.random_masking(ts_processed, ts_length)\n",
    "\n",
    "        output = {\"bert_input\": ts_masking, # 加噪声的时间序列 (72,1)\n",
    "                  \"bert_target\": ts_processed, # 原来未加噪声的时间序列 (72,1)\n",
    "                  \"bert_mask\": bert_mask, # 有数据的地方是1（长度ts_length）,其他地方是0（全长seq_len） (72,)\n",
    "                  \"loss_mask\": mask, # 只计算加噪声位置的loss (72,),加噪声的位置是1,其余位置是0\n",
    "                  \"class_label\": class_label # (1,)\n",
    "                  }\n",
    "\n",
    "        return {key: torch.from_numpy(value) for key, value in output.items()}\n",
    "\n",
    "\n",
    "    def random_masking(self, ts, num_words):\n",
    "\n",
    "        ts_masking = ts.copy()\n",
    "        mask = np.zeros((self.seq_len,), dtype=int)\n",
    "\n",
    "        for i in range(num_words):\n",
    "            prob = random.random()\n",
    "            if prob < 0.2:\n",
    "                prob /= 0.2\n",
    "                mask[(i * self.word_len):(i * self.word_len + self.word_len)] = 1\n",
    "\n",
    "                if prob < 0.5:\n",
    "                    ts_masking[(i * self.word_len):(i * self.word_len + self.word_len), :] \\\n",
    "                        += np.random.uniform(low=-0.5, high=0, size=(self.dimension,))\n",
    "                else:\n",
    "                    ts_masking[(i * self.word_len):(i * self.word_len + self.word_len), :] \\\n",
    "                        += np.random.uniform(low=0, high=0.5, size=(self.dimension,))\n",
    "\n",
    "        return ts_masking, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper(object):\n",
    "    \"\"\"划分训练集和验证集\"\"\"\n",
    "    def __init__(self, batch_size, valid_ratio, data_path, num_features, max_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_ratio = valid_ratio\n",
    "        self.data_path = data_path\n",
    "        self.num_features = num_features\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def get_data_loaders(self):\n",
    "        dataset = PretrainDataset(self.data_path, self.num_features, self.max_length)\n",
    "        train_loader, valid_loader = self.get_train_valid_data_loaders(dataset)\n",
    "        return train_loader, valid_loader\n",
    "    \n",
    "    def get_train_valid_data_loaders(self, dataset):\n",
    "        num_data = len(dataset) # 1463\n",
    "        indices = list(range(num_data))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        valid_split = int(np.floor(self.valid_ratio * num_data))\n",
    "        print('training samples: %d, validation samples: %d' % (num_data-valid_split, valid_split))\n",
    "        train_idx, valid_idx = indices[valid_split:], indices[:valid_split]\n",
    "        \n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "        \n",
    "        train_loader = DataLoader(dataset, batch_size=self.batch_size, sampler=train_sampler,\n",
    "                                 drop_last=True)\n",
    "        valid_loader = DataLoader(dataset, batch_size=self.batch_size, sampler=valid_idx_sampler,\n",
    "                                 drop_last=True)\n",
    "        \n",
    "        return train_loader, valid_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.6.0]",
   "language": "python",
   "name": "conda-env-pytorch1.6.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
